{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89657ffb",
   "metadata": {},
   "source": [
    "# BEQ processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80917884",
   "metadata": {},
   "source": [
    "Walkthrough of a proof of concept taking a set of epub files as input and rendering a corpus of XML-TEI files just as the Lexicoscope 2.0 wants them.\n",
    "\n",
    "Step 0 involves actually getting the ePub, doc or docx files to process. \n",
    "Step 1 involves a set of imports and declarations for the code to work\n",
    "Step 2 involves converting files from epub to XML\n",
    "Step 3 involves converting files from doc/docx to XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c98f5",
   "metadata": {},
   "source": [
    "# Step 1 : Preparations\n",
    "Preparations involve importing the libraries needed, as well as any installations needed. Then we declare the functions we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f417db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 imports : we'll be using these libraries\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974bddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 installs that may be needed \n",
    "\n",
    "##! pip install ebooklib\n",
    "#! pip install docx\n",
    "#! pip install python-docx\n",
    "#! brew install --cask libreoffice\n",
    "#! libreoffice --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd11051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 functions\n",
    "## functions for step2::\n",
    "def epub_to_xml(epub_file):\n",
    "    '''\n",
    "    Transform an epub into an xml file iterating over xml elements\n",
    "    Input: \n",
    "        epub_file : string : absolute path to an epub file to process\n",
    "    Returns :\n",
    "        No return object. An xml file is written to the same directory as the source file with a modified extension.\n",
    "    '''\n",
    "\n",
    "    # Use the input file as the basis of the name of the output file\n",
    "    output_xml = epub_file.replace('.epub','v2.xml')\n",
    "\n",
    "    \n",
    "    # Read the epub file and create an XML root element\n",
    "    book = epub.read_epub(epub_file)\n",
    "    root = etree.Element(\"book\")\n",
    "\n",
    "    # iterate over the spine of the book element, usign the BS parser to parse each item\n",
    "    for item_id, _ in book.spine:\n",
    "      item = book.get_item_with_id(item_id)\n",
    "    \n",
    "      if item and item.media_type == \"application/xhtml+xml\":\n",
    "\n",
    "        soup = BeautifulSoup(item.get_content(), features=\"xml\")\n",
    "        \n",
    "        # Create a new div block for each chapter\n",
    "        div = etree.SubElement(root, \"div\", {\"class\": \"chapter\"})\n",
    "\n",
    "        # get the text of these tag elements,  make a new p element, then add the text to this p element\n",
    "        for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"b\", \"i\", \"strong\", \"em\"]):\n",
    "            if tag.name == \"p\":\n",
    "                p = etree.SubElement(div, \"p\")\n",
    "                p.text = tag.get_text()\n",
    "            # TO TO : this seems unnecessaary with the h elements listed above\n",
    "            elif tag.name in [\"h1\", \"h2\", \"h3\"]:\n",
    "                # Treat headings as chapter div markers\n",
    "                div = etree.SubElement(root, \"div\", {\"class\": \"chapter\"})\n",
    "                title = etree.SubElement(div, \"title\")\n",
    "                title.text = tag.get_text()\n",
    "            ## ensure that b, string, i and em elements are rendered as either b or i elements\n",
    "            elif tag.name in [\"b\", \"strong\"]:\n",
    "                b = etree.SubElement(div, \"b\")\n",
    "                b.text = tag.get_text()\n",
    "            \n",
    "            elif tag.name in [\"i\", \"em\"]:\n",
    "                i = etree.SubElement(div, \"i\")\n",
    "                i.text = tag.get_text()\n",
    "        \n",
    "        # Remove links & references\n",
    "        for a_tag in soup.find_all(\"a\"):\n",
    "            a_tag.extract()\n",
    "    \n",
    "    \n",
    "    # Convert to XML string\n",
    "    xml_str = etree.tostring(root, pretty_print=True, encoding=\"utf-8\").decode()\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_xml, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(xml_str)\n",
    "\n",
    "    print(f\"XML saved to {output_xml}\")\n",
    "    \n",
    "\n",
    "## functions for step3\n",
    "def docx_to_xml(docx_path, xml_path):\n",
    "    \"\"\"Parses .docx and writes XML version\"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    root = etree.Element(\"document\")\n",
    "\n",
    "    for i, para in enumerate(doc.paragraphs):\n",
    "        p_el = etree.SubElement(root, \"paragraph\", index=str(i))\n",
    "        p_el.text = para.text\n",
    "\n",
    "    tree = etree.ElementTree(root)\n",
    "    tree.write(xml_path, pretty_print=True, xml_declaration=True, encoding=\"utf-8\")\n",
    "\n",
    "def doc_to_xml(doc_path):\n",
    "    '''\n",
    "    Convert doc files to xml\n",
    "    Inputs:\n",
    "        doc_path : string : absolute path to a doc file to be converted\n",
    "    Returns :\n",
    "        No return object. On success, an xml file will be created at `xml_path` : `xml_path` will be printed to the console\n",
    "    \n",
    "    '''\n",
    "    # Ensure file exists\n",
    "    if not os.path.isfile(doc_path):\n",
    "        raise FileNotFoundError(f\"No such file: {doc_path}\")\n",
    "\n",
    "    \n",
    "    def convert_doc_to_docx(doc_path):\n",
    "        \"\"\"\n",
    "        Convert a doc file to docx\n",
    "        Inputs:\n",
    "            doc_path : str : absolute path to a doc file to be read\n",
    "        \"\"\"\n",
    "        subprocess.run([\n",
    "        \"/Applications/LibreOffice.app/Contents/MacOS/soffice\",\n",
    "            \"--headless\",\n",
    "            \"--convert-to\",\n",
    "            \"docx\",\n",
    "            doc_path,\n",
    "            \"--outdir\",\n",
    "            os.path.dirname(doc_path)\n",
    "        ], check=True)\n",
    "        \n",
    "        \n",
    "    # Build paths for docx file to be created as an interim format, and the output xml file\n",
    "    base, _ = os.path.splitext(doc_path)\n",
    "    docx_path = base + \".docx\"\n",
    "    xml_path = base + \".xml\"\n",
    "\n",
    "    # Convert .doc to .docx\n",
    "    convert_doc_to_docx(doc_path)\n",
    "\n",
    "    # Convert .docx to .xml\n",
    "    docx_to_xml(docx_path, xml_path)\n",
    "\n",
    "    print(f\"XML saved to: {xml_path}\")\n",
    "\n",
    "\n",
    "def docx_source_to_xml(docx_path):\n",
    "    '''\n",
    "    Convert a docx file to an xml file\n",
    "    Inputs:\n",
    "        docx_path : str: absolute path to a docx file to convert\n",
    "    Returns :\n",
    "        No return object : a file will be exported to `xml_path` : this path will be printed in console on success.\n",
    "    '''\n",
    "    # build the output path from the input path\n",
    "    xml_path = docx_path.replace('.docx','.xml')\n",
    "\n",
    "    # Convert .docx to .xml\n",
    "    docx_to_xml(docx_path, xml_path)\n",
    "\n",
    "    print(f\"XML saved to: {xml_path}\")\n",
    "\n",
    "    \n",
    "# functions for step 4:\n",
    "def p_to_s_as_w(p_blocks, tok_count):\n",
    "  '''\n",
    "  extract the text from p blocks and attach tokens as w children of this p block to tokenise. The additional attribute `EOS` it set to True for tokens in the list of canonical end-of-sentence punctuation, eos_els..\n",
    "  Inputs :\n",
    "      p_blocks: list : a list of etree elements. \n",
    "      tok_count : int : token counter, reset to 0 for each text\n",
    "  Reutrns : \n",
    "      No return object. The etree elements are modified in situ.\n",
    "  ## TO DO : update this tokeniser with the tokeniser function, pool processor… \n",
    "  '''\n",
    "  prev_tok = \"_\"\n",
    "  # define a list of elements that mark the end of sentences\n",
    "  eos_els = ('…',r'?','!',r'.',r'\\n')\n",
    "  excl_list = ()#('1','2','3','4','5','6','7','8','9','0', 'L', \"M\", \"R\")\n",
    "  ## iterate over the p blocks, getting the text and tidying it\n",
    "  for p_block in tqdm(p_blocks):\n",
    "  # p_block = p_blocks[134]\n",
    "    current_parent = p_block.getparent()\n",
    "    line_raw = ''.join([chunk for chunk in p_block.itertext()]).strip()\n",
    "    line_raw = re.sub(r'\\.\\.\\.',' … ', line_raw)\n",
    "    line_tok = re.sub(r'([\\?\\!\\(\\)\\.,:;\\\"])', r' \\1 ', line_raw)\n",
    "    line_tok = re.sub(r'[\\'’]', r\"' \", line_tok)\n",
    "    line_tok = re.sub(r\"aujourd' hui\", r\"aujourd'hui\", line_tok)\n",
    "    line_tok = re.sub(r\"rud' homm\", r\"rud'homm\", line_tok)\n",
    "    line_tok = re.sub(r'(-je|-tu|-il|-elle|-on|-ça|-cela|-nous|-vous|-ils|-elles|-moi|-toi|-lui|-leur|-en|-y|-ilz)',r' \\1',line_tok)\n",
    "    line_tok = re.sub('  ', ' ', line_tok )\n",
    "    line_tok = re.sub('  ', ' ', line_tok )\n",
    "    line_tok = re.sub('  ', ' ', line_tok )\n",
    "    line_tok = re.sub('  ', ' ', line_tok )\n",
    "    line_tok = re.sub('\\t\\t','\\t',line_tok)\n",
    "    line_tok = re.sub('(\\n\\n)+','\\n',line_tok)\n",
    "    line_tidy = re.sub(r'^ | $','', line_tok) \n",
    "    \n",
    "    # when text is tidied, split the text on spaces to yield tokens\n",
    "    all_toks = line_tidy.split(' ')\n",
    "    ## for each token, nake a w element as a child of the p element, and add the token count as an attribute.\n",
    "    for tok in all_toks:\n",
    "      tok_count +=1\n",
    "      w_el = etree.Element('w')\n",
    "      w_el.set('id', str(tok_count))\n",
    "      w_el.text = tok\n",
    "      ## if the token is an \n",
    "      if tok in eos_els and prev_tok not in excl_list:\n",
    "        w_el.set('EOS','True')\n",
    "      # print(tok, tok_count, tok in eos_els)\n",
    "      prev_tok = tok\n",
    "      p_block.append(w_el)\n",
    "    p_block.text = \"\"  \n",
    "\n",
    "def run_sentencisation_modifier(input_file):\n",
    "  '''\n",
    "  run the sentencisation processor over the tokenised files to attribute tokens to sentences and adjust sentence boundaries based on the presence of further punctuation marks\n",
    "  Inputs:\n",
    "      input_file : string : absolute path to an xml file to be sentencised\n",
    "  Returns :\n",
    "      No return object. A file is exported in the same directory as the input file, with an incremented suffix : v2 in, v3 out.\n",
    "  '''\n",
    "  # parse the file as a tree and get the list of w elements\n",
    "  input_tree = etree.parse(input_file)\n",
    "  w_elements = list(input_tree.iter(\"w\"))\n",
    "  s_count =1\n",
    "  # attach each word to a sentence by giving it a sentnum attribute and value\n",
    "  for w in tqdm(w_elements):\n",
    "      w.set(\"sentnum\", str(s_count))\n",
    "      if w.text == \"…\" and 'EOS' in w.attrib.keys() :\n",
    "        del w.attrib['EOS']\n",
    "      # If this <w> element has EOS=\"true\", finalize the current <s> and start a new one\n",
    "      if w.get(\"EOS\") == \"True\" or w.text in (':',';'):\n",
    "        w.set(\"sentnum\", str(s_count))\n",
    "        s_count +=1 \n",
    "  input_tree.write(input_file.replace('v2.xml','v3.xml'), encoding='UTF-8', pretty_print=True)\n",
    "\n",
    "\n",
    "    \n",
    "def group_w_by_num_and_wrap_with_s(input_tree):\n",
    "    '''\n",
    "    Build a dictionary of of w elements where sent numbers are keys then use this dictionary to make s elements that are parents of w elements\n",
    "    Inputs :\n",
    "        input_tree : etree : an lxml etree\n",
    "    Returns :\n",
    "        No return object. `input_tree` is modified in place and then exported to the same directory as the source file, with th esuffix incremented from v3 to v4\n",
    "    '''  \n",
    "    root = input_tree.getroot()\n",
    "    # Step 1: Collect all <w> elements grouped by num\n",
    "    sentnum_to_w = defaultdict(list)\n",
    "    w_to_parent = {}\n",
    "\n",
    "    for parent in root.iter():\n",
    "        for w in list(parent):  # Convert to list to safely modify tree\n",
    "            if w.tag == 'w' and 'sentnum' in w.attrib:\n",
    "                sentnum = w.attrib['sentnum']\n",
    "                sentnum_to_w[sentnum].append(w)\n",
    "                w_to_parent[w] = parent\n",
    "\n",
    "    # Step 2: Wrap groups with same num in <s>\n",
    "    for sentnum, w_elements in sentnum_to_w.items():\n",
    "        if len(w_elements) <= 1:\n",
    "            continue  # Only wrap if group has more than one <w>\n",
    "\n",
    "        # Assume all have same parent\n",
    "        parent = w_to_parent[w_elements[0]]\n",
    "\n",
    "        # Create new <s> element\n",
    "        s_elem = etree.Element('s')\n",
    "        s_elem.attrib['sentnum'] = sentnum\n",
    "\n",
    "        # Find index of first <w> to insert <s> in correct position\n",
    "        insert_index = parent.index(w_elements[0])\n",
    "        parent.insert(insert_index, s_elem)\n",
    "\n",
    "        for w in w_elements:\n",
    "            try:\n",
    "              parent.remove(w)\n",
    "              s_elem.append(w)\n",
    "            except Exception as e :\n",
    "              s_elem.append(w)\n",
    "              \n",
    "    input_tree.write(input_file.replace('v3.xml','v4.xml'), encoding='UTF-8', pretty_print=True)\n",
    "\n",
    "def make_xmlconllu(input_file):\n",
    "  '''\n",
    "  Transform an xml document into an xml-conllu document by making a series of raw conllu strings for each sentence\n",
    "  Inputs:\n",
    "      input_file : string : absolute path to an xml file\n",
    "  Returns:\n",
    "      No return object. An xml file is printed with the xml-conllu strings as the text attribute of sentence elements. The file is printed tot the same directory as the input, with the suffix incremented from v4 to v5\n",
    "\n",
    "  '''\n",
    "  # parse the xml \n",
    "  input_tree = etree.parse(input_file)\n",
    "  w_els = input_tree.findall(\".//w\")\n",
    "  # and get the w elements and delete the sentnum attribute which is now redundant\n",
    "  for w_el in w_els:\n",
    "      del w_el.attrib['sentnum']\n",
    "  # get the s elements which contain the sentences and iterate over them\n",
    "  s_blocks = input_tree.findall(\".//s\")\n",
    "  for s_block in s_blocks:\n",
    "    # make a new list for each sentence, adding the sentence numnber as the sent_id in a conll meta line\n",
    "    outputlist = []\n",
    "    metaline = f'\\n\\n# sent_id = {s_block.get(\"sentnum\")}\\n'\n",
    "    outputlist.append(metaline)\n",
    "    ## iterate over the tokens in the sentence, converting them to conll strings and appending them to the list of strings that are the sentence\n",
    "    for n, w_block in enumerate(s_block.findall(\".//w\"), start=1):\n",
    "      left = f'{str(n)}\\t{w_block.text}'\n",
    "      right = f\"\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\tw_{w_block.get('id')}\"\n",
    "      line = f'{left}{right}\\n'\n",
    "      outputlist.append(line)  \n",
    "    ## join all the strings into 1  large string which constitutes the entire sentence as 1 valid conll string, and add this as the text of the s element\n",
    "    s_conll = \"\".join([item for item in outputlist])\n",
    "    s_block.text = s_conll\n",
    "  input_tree.write(input_file.replace('v4','v5'), encoding='UTF-8-', pretty_print=True)\n",
    "\n",
    "\n",
    "def make_conll_docs(input_file):\n",
    "  '''\n",
    "  Take an xml-conllu file and extract the conll strings to a validated conll document\n",
    "  Inputs : \n",
    "    input_file : string : absolute path to the same input file as `make_xmlconllu` - the path will be automatically modified to point to the xml-cnllu file createdby this funciton.\n",
    "  Returns :\n",
    "    No return object. The function to the same directory as the input, with the extension changed to .conllu\n",
    "  \n",
    "  '''\n",
    "  input_file = input_file.replace('.v4xml','.v5xml')\n",
    "  input_tree = etree.parse(input_file)\n",
    "  outputfile = input_file.replace('.xml','.conllu')\n",
    "  s_chunks = input_tree.findall(\".//s\")\n",
    "  output = []\n",
    "  for s_chunk in s_chunks:\n",
    "    conll_lines = s_chunk.text\n",
    "    output.append(conll_lines)\n",
    "  doc = CoNLL.conll2doc(input_str = \"\".join([item for item in output]))\n",
    "  with open(outputfile, 'w', encoding='UTF-8') as c:\n",
    "    for chunk in output:\n",
    "      _ = c.write(chunk)\n",
    "\n",
    "def reinsert_conll_annots(conll_annotation_file):\n",
    "  '''\n",
    "  Reinsert conll annotations into an existing xml-conll document\n",
    "  Inputs:\n",
    "      conll_annotation_file : string : absolute path to the file of conllu annotations from the parser\n",
    "  Returns :\n",
    "      No return object. The function will print an XML-conllu file to the same location as the conllu and xml-conllu files with the the xml extension and the suffix incremented from v6 to v6.\n",
    "  '''  \n",
    "\n",
    "  # building filepaths for the source xml file based on the inputconll file, and the output file to print to by incrementing the suffix\n",
    "  target_xmlfile = input_conllfile.replace('_out.conllu','.xml').replace('07_conlluout','05_v5xml')\n",
    "  output_xmlconllu = target_xmlfile.replace('05_v5xml','08_xmlconllu').replace('v5.xml','v6.xml')\n",
    "\n",
    "  # loads the xml tree and build a dictionary of sentence numbers and elements\n",
    "  input_tree = etree.parse(target_xmlfile)\n",
    "  target_dict = {element.get(\"sentnum\"):element for element in input_tree.findall(\".//s\")}\n",
    "\n",
    "  # load the annotated conll doc and iterate over sentences\n",
    "  conll_doc = CoNLL.conll2doc(input_conllfile)\n",
    "  for sent in conll_doc.sentences:\n",
    "    # remove the head from the conll meta line to yield the sentence number, which is the key in `target_dict`\n",
    "    sent_idchunk = sent.comments[0].replace('# sent_id = ','')\n",
    "    target_el = target_dict.get(sent_idchunk)\n",
    "    # concatenate the conll text into a single string with the necessary line breaks to represent the sentence and add this as the attribute of the s element\n",
    "    target_el.text =   \"\\n\".join([token.to_conll_text() for token in sent.tokens])\n",
    "  \n",
    "  # remove the w elements from the tree, as we won't use them again, then print the tree to file\n",
    "  # prune tree\n",
    "  for w in input_tree.findall(\".//w\"):\n",
    "    parent = w.getparent()\n",
    "    parent.remove(w)\n",
    "  input_tree.write(output_xmlconllu, encoding='UTF-8', pretty_print=True)\n",
    "  \n",
    "\n",
    "def make_bibdict(biblio_file):\n",
    "    '''\n",
    "    make a dictionary from an html file to automate adding bibliographical into to TEI headers\n",
    "    Inputs :\n",
    "        biblio_file : str : absolute file path to an html file of bibliographic data prepared for header creation\n",
    "    Returns :\n",
    "        bib_dict : dict : a python dictionary of the bibliographical information with filenames as keys\n",
    "    '''\n",
    "    bib_dict = {}\n",
    "    biblio_file = '/Users/username/Downloads/beq/biblio.html'\n",
    "    bib_tree = etree.parse(biblio_file)\n",
    "    for author in bib_tree.findall(\".//author\"):\n",
    "      author_name = author.findall(\".//name\")[0].text\n",
    "      print(author_name)\n",
    "      works = author.findall(\".//work\")\n",
    "      for work in works:\n",
    "        filename = work.get(\"version\").replace('.pdf','').replace('.doc','')  \n",
    "        title = work.get(\"title\")\n",
    "        data = author_name, title\n",
    "        key = filename\n",
    "        bib_dict[key] = data\n",
    "    return bib_dict\n",
    "    \n",
    "def make_and_insert_headers(xml_conllufile, bib_dict, error_list):    \n",
    "  '''\n",
    "  Make teiHeaders based on a custom dictionary, inserting the header into a XML-conllu file which is exported.\n",
    "  Inputs :\n",
    "    xml_conllufile : string : absolute path to an xml_conllu file created as part of this pipeline\n",
    "    bib_dict : dictionary : a python dictionary with filenames as keys and author-title information in subdicts\n",
    "    error_list : list : a list of input files which there were errors during processing\n",
    "  Returns : \n",
    "    No return object. An XML-CONLL file will be printed on success, otherwise, the filename will be added ot the error_list\n",
    "\n",
    "  '''\n",
    "    # raw text to parse into an etree element to define header    \n",
    "  empty_header = '''\n",
    "        <teiHeader>\n",
    "            <fileDesc>\n",
    "                <titleStmt>\n",
    "                    <title></title>\n",
    "                    <date></date>\n",
    "                    <author></author>\n",
    "                    <respStmt>\n",
    "                        <name />\n",
    "                        <resp />\n",
    "                    </respStmt>\n",
    "                    <respStmt>\n",
    "                        <name />\n",
    "                        <resp />\n",
    "                    </respStmt>\n",
    "                </titleStmt>\n",
    "                <publicationStmt>add custom string here</publicationStmt>\n",
    "                        <profileDesc>\n",
    "            <langUsage>\n",
    "                <language ident=\"fr\" />\n",
    "            </langUsage>\n",
    "      </profileDesc>\n",
    "            </fileDesc>\n",
    "        </teiHeader>\n",
    "  '''\n",
    "  # make filenames based on replacements file names and paths\n",
    "  outputname = xml_conllufile.replace('08_xmlconllu','09_xmlconllu').replace('v6.xml','v7.xml')\n",
    "  filename = xml_conllufile.replace(path, '').replace('v6.xml','')\n",
    "\n",
    "  # parse the empty header and the input xml-conllu file\n",
    "  teiheader = etree.fromstring(empty_header)\n",
    "  input_tree = etree.parse(xml_conllufile)\n",
    "\n",
    "  # get the root and chante its tag to text\n",
    "  root = input_tree.getroot()\n",
    "  root.tag = \"text\"\n",
    "  ## iterate over s blocks, setting the id attribute to the value of the sentnum attribute, then delete the sentnum attribute\n",
    "  for sblock in root.findall(\".//s\"):\n",
    "    sblock.set(\"id\", str(sblock.get('sentnum')))\n",
    "    del sblock.attrib['sentnum']\n",
    "\n",
    "  # getting the author, title info from the dictionary based on the filename. Initially, try on the filename as it is ; if there is no match, try splitting the filename on th ehyphen, as some filenames contain both the author name adn the title.\n",
    "  try:\n",
    "    author, title = bib_dict[filename]\n",
    "  except KeyError:\n",
    "    errorlist.append(filename)\n",
    "    try:\n",
    "      author, title= filename.split(\"-\")\n",
    "    except ValueError:\n",
    "      errorlist.append(filename)\n",
    "      author, title= filename, filename\n",
    "    \n",
    "  # set the author and title attributes in the title statement\n",
    "  teiheader.findall(\".//titleStmt/title\")[0].text = title\n",
    "  teiheader.findall(\".//titleStmt/author\")[0].text = author\n",
    "  # make a new top level element, and add append header and root elements to it\n",
    "  new_tree = etree.Element(\"TEI.2\")\n",
    "  new_tree.append(teiheader)\n",
    "  new_tree.append(root)\n",
    "  \n",
    "  # convert the new top level element into an Element tree, then print to file\n",
    "  output_tree = etree.ElementTree(new_tree)\n",
    "  output_tree.write(outputname, encoding='UTF-8', pretty_print=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b32ea",
   "metadata": {},
   "source": [
    "Once these cells have been run, step 1 is done, and we're ready to actually process some files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718115bc",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Step 2 has three parts, to deal with ePub files, doc files and docx files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1  get a folder of epub files : I put them in a folder of epubs here :\n",
    "epub_files = glob.glob(f'/Users/{username}/Downloads/beq/epub/*.epub')\n",
    "\n",
    "# Run the converter to convert the epub files to XML\n",
    "for epub_file in tqdm(epub_files):\n",
    "  epub_to_xml(epub_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# In step 3, we need to process documents according to their filetype. First, we'll run the processor for the docx files\n",
    "\n",
    "# convert docX files to xml\n",
    "docx_files = glob.glob(f'/Users/{username}/Downloads/beq/doc/*.docx')\n",
    "for docx_path in tqdm(docx_files):\n",
    "  docx_source_to_xml(docx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3\n",
    "# And then we'll run the special processor for the doc files.\n",
    "\n",
    "doc_filelist = glob.glob(f'/Users/{username}/Downloads/beq/doc/*.doc')\n",
    "for input_file in tqdm(doc_filelist):\n",
    "  doc_to_xml(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbcbdf",
   "metadata": {},
   "source": [
    "This now ends step 2 : we've got a folder of XML files extracted from epub, doc and docx files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be61f9",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "In step3 we'll tokenise : ie turn our paragraphs into words, for all the XML files we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_files = glob.glob(f'/Users/{username}/Downloads/beq/epub/*.xml')\n",
    "for current_file in current_files:\n",
    "  current_tree = etree.parse(current_file)\n",
    "  p_blocks = current_tree.findall(\".//p\")\n",
    "  tok_count = 0\n",
    "  p_to_s_as_w(p_blocks, tok_count)\n",
    "  current_tree.write(current_file.replace('.xml','v2.xml'), encoding='UTF-8', pretty_print=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45cc0e",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "In step 4, we'll sentencise: say where the sentence boundaries are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f451bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_files = glob.glob(f'/Users/{username}/Downloads/beq/epub/*v2.xml')\n",
    "for input_file in tqdm(input_files):\n",
    "    run_sentencisation_modifier(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca69da9",
   "metadata": {},
   "source": [
    "# Step5\n",
    "In step 5, we'll add sentences as an explicit level of tags between p and w based on the attributes, to get the needed XML structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   use the sent numbers to define sentences and create s elements as parents of w and children of p\n",
    "## now have all w element siwht w id, and sent_num, and EOS attrib. need to it=erate over tree\n",
    "root = input_tree.getroot()\n",
    "\n",
    "input_files = glob.glob(f'/Users/{username}/Downloads/beq/epub/*v3.xml')\n",
    "for input_file in tqdm(input_files):\n",
    "  input_tree = etree.parse(input_file)\n",
    "  group_w_by_num_and_wrap_with_s(input_tree)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461284a",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "In step 6, we convert the XML to XML-conllu, and extract just the conllu to send it to the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### next step: XML to xml-conllu and conllu\n",
    "input_files = glob.glob(f'/Users/{username}/Downloads/beq/epub/*v4.xml')\n",
    "# tidy w blocks, make s_conllu\n",
    "for input_file in tqdm(input_files):\n",
    "    make_xmlconllu(input_file)\n",
    "    make_conll_docs(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe24034",
   "metadata": {},
   "source": [
    "# Step7\n",
    "In step 7, the conll documents are parsed by  parser, such as Stanza (not documented here, as it's entirely standard processing to feed Stanza models conll documents and get conll documents out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884ff20",
   "metadata": {},
   "source": [
    "# Step8\n",
    "In step 8, we get the annotated conll files from the parser and reinsert them back into the XML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8 sent conll annotations back to xml  \n",
    "input_conllfiles = glob.glob(f'/Users/{username}/Downloads/beq/epub/07_conlluout/*.conllu')\n",
    "for conll_annotation_file in tqdm(input_conllfiles):\n",
    "    reinsert_conll_annots(conll_annotation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b89762",
   "metadata": {},
   "source": [
    "# Step 9 \n",
    "In step 9, we add headers to the XML files based on a custom dictionary and export the XML-conll after some final structuring of the XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#step 10:   build a dictionary of bibliographic data from html file\n",
    "biblio_file = f'/Users/{username}/Downloads/beq/biblio.html'\n",
    "bib_dict = make_bibdict(biblio_file)\n",
    "path = '/Users/Adam/Downloads/beq/epub/08_xmlconllu/'\n",
    "errorlist =[]\n",
    "xml_conllufiles = glob.glob(f'/Users/{username}/Downloads/beq/epub/08_xmlconllu/*.xml')\n",
    "for xml_conllufile in tqdm(xml_conllufiles):\n",
    "    make_and_insert_headers(xml_conllufile, bib_dict, error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c08d62",
   "metadata": {},
   "source": [
    "Done ! We've not got a corpus of texts formatted for the LExicoscope to ingest as xml-conllu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
